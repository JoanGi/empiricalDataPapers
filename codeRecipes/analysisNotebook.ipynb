{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting and parsing sources (stored as a list of Document objects in strings)\n",
    "sources = pd.read_excel(\"../results/sources.xlsx\")\n",
    "dimensions = {\n",
    "\"uses_uses_sources\",\n",
    "\"uses_data_limits_sources\",\n",
    "\"uses_ml_approach_sources\",\n",
    "\"uses_represents_people_sources\",\n",
    "\"uses_biases_sources\",\n",
    "\"uses_privacy_sources\",\n",
    "\"uses_sensitivity_sources\",\n",
    "\"uses_maintenance_policies_sources\",\n",
    "\"collection_explanation_sources\",\n",
    "\"collection_team_sources\",\n",
    "\"collection_labour_sources\",\n",
    "\"collection_team_demographic_sources\",\n",
    "\"collection_target_demographics_sources\",\n",
    "\"collection_speakers_demographics_sources\",\n",
    "\"collection_sources_sources\",\n",
    "\"collection_infrastructure_sources\",\n",
    "\"annotation_explanation_sources\",\n",
    "\"annotation_team_demographi_sources\",\n",
    "\"annotation_infrastructure_sources\",\n",
    "\"annotation_validation_methods_sources\",\n",
    "}\n",
    "source_sections = []\n",
    "\n",
    "\n",
    "for idx, source in sources.iterrows():\n",
    "    row = {\"doi\":source['doi']}\n",
    "    for dimension in dimensions:\n",
    "        if  isinstance(source[dimension],str):\n",
    "            documents = eval(source[dimension])\n",
    "            sections = []\n",
    "            for document in documents:\n",
    "                if dimension == \"uses_uses_sources\":\n",
    "                    sections.insert(0,document[0].metadata['source'])\n",
    "                else:\n",
    "                    sections.insert(0,document.metadata['source'])\n",
    "            row[dimension+\"_sections\"] = sections\n",
    "    source_sections.append(row)\n",
    "    pdSection = pd.DataFrame(source_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading raw data\n",
    "rawData = pd.read_excel(\"../results/FullStudyAnalysis.xlsx\", sheet_name=\"Raw Data\")\n",
    "# Joining sources to raw data\n",
    "mergedData = rawData.merge(sources, on=\"doi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dimensions subsets from our raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is using an ML approach\n",
    "subsetML = mergedData.query(\"uses_ml_approach_bool == 'Yes'\")\n",
    "# Use data limits\n",
    "subsetLimits = mergedData.query(\"uses_data_limits.str.contains('Yes,*')\")\n",
    "# Social Concerns subset\n",
    "strng = \"<class 'str'>\"\n",
    "subsetPeople = mergedData.query(\"uses_biases_bool != @strng\")\n",
    "subsetSocial = mergedData.query(\"uses_biases_bool == 'Yes'\")\n",
    "# Collection team demographics\n",
    "subsetCollectionTeam = mergedData.query(\"collection_team_demographic.str.contains('Yes,*')\")\n",
    "# Collection target demographics\n",
    "subsetCollectionTargetDemographics = mergedData.query(\"collection_target_demographics.str.contains('Yes,*',na=False)\")\n",
    "# Collection speakers demographics\n",
    "subsetCollectionSpeakersDemographics = mergedData.query(\"collection_speakers_demographics.str.contains('Yes,*',na=False)\")\n",
    "# Annotation profile\n",
    "subsetAnntationTeam = mergedData.query(\"annotation_team_demograaphic.str.contains('Yes,*',na=False)\")\n",
    "# Annotation infrastructure\n",
    "subsetAnntationInfrastructure = mergedData.query(\"annotation_infrastructure.str.contains('Yes,*',na=False)\")\n",
    "# Annotation validation\n",
    "subsetAnntationValidation = mergedData.query(\"annotation_validation_methods.str.contains('Yes,*',na=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(dataPapersList, dimension):\n",
    "    sections = []\n",
    "    for idx, papers in dataPapersList.iterrows():\n",
    "        if  isinstance(papers[dimension],str):\n",
    "            documents = eval(papers[dimension])\n",
    "            for document in documents:\n",
    "                    if dimension == \"uses_uses_sources\":\n",
    "                        sections.insert(0,document[0].metadata['source'])\n",
    "                    else:\n",
    "                        sections.insert(0,document.metadata['source'])\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "countsUses = pd.DataFrame(extract_sections(mergedData,\"uses_uses_sources\")).value_counts()\n",
    "countsMLapproach = pd.DataFrame(extract_sections(subsetML,\"uses_ml_approach_sources\")).value_counts()\n",
    "countsLimits = pd.DataFrame(extract_sections(subsetLimits,\"uses_data_limits_sources\")).value_counts()\n",
    "countsSocial = pd.DataFrame(extract_sections(subsetSocial,\"uses_biases_sources\")).value_counts()\n",
    "countsCollection = pd.DataFrame(extract_sections(mergedData,\"collection_explanation_sources\")).value_counts()\n",
    "countsColTeam = pd.DataFrame(extract_sections(subsetCollectionTeam,\"collection_team_demographic_sources\")).value_counts()\n",
    "countsColTarget = pd.DataFrame(extract_sections(subsetCollectionTargetDemographics,\"collection_target_demographics_sources\")).value_counts()\n",
    "countsCollectionSources = pd.DataFrame(extract_sections(mergedData,\"collection_sources_sources\")).value_counts()\n",
    "countsAnnotation = pd.DataFrame(extract_sections(mergedData,\"annotation_explanation_sources\")).value_counts()\n",
    "countsAnnTeam = pd.DataFrame(extract_sections(subsetAnntationTeam,\"annotation_team_demographi_sources\")).value_counts()\n",
    "countsAnnInfr = pd.DataFrame(extract_sections(subsetAnntationInfrastructure,\"annotation_infrastructure_sources\")).value_counts()\n",
    "countsAnnVal = pd.DataFrame(extract_sections(subsetAnntationValidation,\"annotation_validation_methods_sources\")).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SData Maching required sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "classifier = transformers.pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "sdataRequiredSections = [\n",
    "\"Title\",\n",
    "\"Abstract\",\n",
    "\"Background & Summary\",\n",
    "\"Methods\",\n",
    "\"Data Records\",\n",
    "\"Technical Validation\",\n",
    "\"Usage Notes (optional)\",\n",
    "\"Code Availability\",\n",
    "\"References\",\n",
    "\"Author Contributions\",\n",
    "\"Competing Interests\",\n",
    "\"Other\"\n",
    "]\n",
    "\n",
    "# Subset by jorunal\n",
    "subsetSData = mergedData.query(\"journal_x == 'SData'\")\n",
    "subsetDBrief = mergedData.query(\"journal_x == 'DBrief'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyLabels(sections):\n",
    "    finalSections = []\n",
    "    for section in sections:\n",
    "        resultClass  = classifier(section, sdataRequiredSections)\n",
    "        finalSections.append(resultClass['labels'][0])\n",
    "        # save the result\n",
    "    return finalSections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countsSDUsesClassified = pd.DataFrame(classifyLabels(extract_sections(mergedData,\"uses_uses_sources\"))).value_counts()\n",
    "countsSDMLapproachClassified = pd.DataFrame(classifyLabels(extract_sections(subsetML,\"uses_ml_approach_sources\"))).value_counts()\n",
    "countsSDLimitsClassified = pd.DataFrame(classifyLabels(extract_sections(subsetLimits,\"uses_data_limits_sources\"))).value_counts()\n",
    "countsSDSocialClassified = pd.DataFrame(classifyLabels(extract_sections(subsetSocial,\"uses_biases_sources\"))).value_counts()\n",
    "countsSDCollectionClassified = pd.DataFrame(classifyLabels(extract_sections(mergedData,\"collection_explanation_sources\"))).value_counts()\n",
    "countsSDColTeamClassified = pd.DataFrame(classifyLabels(extract_sections(subsetCollectionTeam,\"collection_team_demographic_sources\"))).value_counts()\n",
    "countsSDColTargetClassified = pd.DataFrame(classifyLabels(extract_sections(subsetCollectionTargetDemographics,\"collection_target_demographics_sources\"))).value_counts()\n",
    "countsSDCollectionSourcesClassified = pd.DataFrame(classifyLabels(extract_sections(mergedData,\"collection_sources_sources\"))).value_counts()\n",
    "countsSDAnnotationClassified = pd.DataFrame(classifyLabels(extract_sections(mergedData,\"annotation_explanation_sources\"))).value_counts()\n",
    "countsSDAnnTeamClassified = pd.DataFrame(classifyLabels(extract_sections(subsetAnntationTeam,\"annotation_team_demographi_sources\"))).value_counts()\n",
    "countsSDAnnInfrClassified = pd.DataFrame(classifyLabels(extract_sections(subsetAnntationInfrastructure,\"annotation_infrastructure_sources\"))).value_counts()\n",
    "countsSDAnnValClassified = pd.DataFrame(classifyLabels(extract_sections(subsetAnntationValidation,\"annotation_validation_methods_sources\"))).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extractSources(dataPapers):\n",
    "    for idx, source in dataPapers.iterrows():\n",
    "        row = {\"doi\":source['doi']}\n",
    "        if  isinstance(source[dimension],str):\n",
    "            documents = eval(source[dimension])\n",
    "            for document in documents:\n",
    "                    if dimension == \"uses_uses_sources\":\n",
    "                        sections.insert(0,document[0].metadata['source'])\n",
    "                    else:\n",
    "                        sections.insert(0,document.metadata['source'])\n",
    "            row[dimension+\"_sections\"] = sections\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uses_uses_sources_sections\n",
       "[Data Records, Data Records, Data Records, Data Records]                                                                                                                                              6\n",
       "[Methods, Methods, Methods, Methods]                                                                                                                                                                  5\n",
       "[Methods, Methods, Methods, Code availability]                                                                                                                                                        5\n",
       "[Usage Notes, Usage Notes, Usage Notes, Usage Notes]                                                                                                                                                  5\n",
       "[Code availability, Methods, Data Records, Usage Notes]                                                                                                                                               4\n",
       "                                                                                                                                                                                                     ..\n",
       "[Author Contributions, Additional Information, Methods, Data Records]                                                                                                                                 1\n",
       "[Additional Information, Data Records, Author Contributions, Technical Validation]                                                                                                                    1\n",
       "[Data de-identification, 3., Data Records, Methods]                                                                                                                                                   1\n",
       "[Additional information, Frogs were exposed to Bd or sham-exposed to DSS, Swabs were collected weekly and pre-euthanasia to determine infection intensities via qPCR (Experiment A), Data Records]    1\n",
       "[, , Technical Validation, Methods]                                                                                                                                                                   1\n",
       "Name: count, Length: 3920, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdSection['uses_uses_sources_sections'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_uses = []\n",
    "for index, paper in pdSection.iterrows():\n",
    "    for section in paper['uses_uses_sources_sections']:\n",
    "        sources_uses.append(section)\n",
    "pdSourcesUses = pd.DataFrame(sources_uses)\n",
    "pdSourcesUses.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame(pdSourcesUses.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source0 = unserialized_sources[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
